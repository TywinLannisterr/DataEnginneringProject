heat_template_version: "2015-04-30"
description: "DE1-Group17 HOT for Spark project"

# Template parameters
parameters:
  network_id:
    type: string
    label: Network ID
    description: Network ID of desired network for the instance
    default: "09e80bd7-27ac-4623-944c-24095b877fc5" #UPPMAX 2022/1
  
  image_id:
    type: string
    label: Image ID
    description: Image to be used for instances
    default: "98c10a7f-2587-450b-866c-1266ea0dbe4b" #Ubuntu 20.04 - 2021.03.23
  
  key_name:
    type: string
    label: Key Name
    description: Name of key-pair to be used for istances
    default: de1
  
  sec_group:
    type: string
    label: Security group
    description: Security group ID for the instance
    default: "c1bb0532-80cd-4192-a0a1-e0f9454aba4a" #default security group

#Instance specification

resources: 
###################################################
########## SPARK MASTER ###########################
###################################################
  spark-master: 
    type: "OS::Nova::Server"
    properties: 
      security_groups: 
        - { get_param: sec_group }
      networks: 
        - network: { get_param: network_id }
      name: G17-spark-master
      flavor: "ssc.medium"
      image: { get_param: image_id }
      availability_zone: nova
      key_name: { get_param: key_name }
      user_data_format: RAW
      user_data: |
        #!/bin/bash
        #Download required packages:
        sudo apt update
        sudo apt install -y openjdk-11-jdk
        sudo apt install -y scala
        sudo apt install -y openssh-server openssh-client

        #Enable ssh to localhost
        ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y
        cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys

        cd /home/ubuntu/
        wget https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz
        tar xvf spark-3.0.3-bin-hadoop2.7.tgz
        sudo mv spark-3.0.3-bin-hadoop2.7 /usr/local/spark 

        export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
        export SPARK_HOME=/usr/local/spark
        export INSTANCE_IP=`hostname -I| awk '$1=$1'`
        echo "export SPARK_MASTER_HOST=$INSTANCE_IP" >> $SPARK_HOME/conf/spark-env.sh
        echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $SPARK_HOME/conf/spark-env.sh

        $SPARK_HOME/sbin/start-master.sh

###################################################
########## SPARK WORKER 1 #########################
###################################################
  spark-worker-1: 
    type: "OS::Nova::Server"
    properties: 
      security_groups: 
        - { get_param: sec_group }
      networks: 
        - network: { get_param: network_id }
      name: G17-spark-worker-1
      flavor: "ssc.medium.highmem"
      image: { get_param: image_id }
      availability_zone: nova
      key_name: { get_param: key_name }
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/bash
            #Download required packages:
            sudo apt update
            sudo apt install -y openjdk-11-jdk
            sudo apt install -y scala

            #Enable ssh to localhost
            ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y
            cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys

            cd /home/ubuntu/
            wget https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz
            tar xvf spark-3.0.3-bin-hadoop2.7.tgz
            sudo mv spark-3.0.3-bin-hadoop2.7 /usr/local/spark

            export SPARK_HOME=/usr/local/spark
            for i in {1..255}; do echo "192.168.1.$i host-192-168-1-$i-de1"| sudo tee -a /etc/hosts; done
            for i in {1..255}; do echo "192.168.2.$i host-192-168-2-$i-de1"| sudo tee -a /etc/hosts; done

            $SPARK_HOME/sbin/start-slave.sh -m 4g spark://$master_ip:7077
          params:
           $master_ip: {get_attr: [spark-master, first_address]}
    depends_on: 
      - spark-master

###################################################
########## SPARK WORKER 2 #########################
###################################################
  spark-worker-2: 
    type: "OS::Nova::Server"
    properties: 
      security_groups: 
        - { get_param: sec_group }
      networks: 
        - network: { get_param: network_id }
      name: G17-spark-worker-2
      flavor: "ssc.medium.highmem"
      image: { get_param: image_id }
      availability_zone: nova
      key_name: { get_param: key_name }
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/bash
            #Download required packages:
            sudo apt update
            sudo apt install -y openjdk-11-jdk
            sudo apt install -y scala

            #Enable ssh to localhost
            ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y
            cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys

            cd /home/ubuntu/
            wget https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz
            tar xvf spark-3.0.3-bin-hadoop2.7.tgz
            sudo mv spark-3.0.3-bin-hadoop2.7 /usr/local/spark

            export SPARK_HOME=/usr/local/spark
            for i in {1..255}; do echo "192.168.1.$i host-192-168-1-$i-de1"| sudo tee -a /etc/hosts; done
            for i in {1..255}; do echo "192.168.2.$i host-192-168-2-$i-de1"| sudo tee -a /etc/hosts; done

            $SPARK_HOME/sbin/start-slave.sh -m 4g spark://$master_ip:7077
          params:
           $master_ip: {get_attr: [spark-master, first_address]}
    depends_on: 
      - spark-master


###################################################
########## SPARK WORKER 3 #########################
###################################################
  spark-worker-3: 
    type: "OS::Nova::Server"
    properties: 
      security_groups: 
        - { get_param: sec_group }
      networks: 
        - network: { get_param: network_id }
      name: G17-spark-worker-3
      flavor: "ssc.medium.highmem"
      image: { get_param: image_id }
      availability_zone: nova
      key_name: { get_param: key_name }
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/bash
            #Download required packages:
            sudo apt update
            sudo apt install -y openjdk-11-jdk
            sudo apt install -y scala

            #Enable ssh to localhost
            ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y
            cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys

            cd /home/ubuntu/
            wget https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz
            tar xvf spark-3.0.3-bin-hadoop2.7.tgz
            sudo mv spark-3.0.3-bin-hadoop2.7 /usr/local/spark

            export SPARK_HOME=/usr/local/spark
            for i in {1..255}; do echo "192.168.1.$i host-192-168-1-$i-de1"| sudo tee -a /etc/hosts; done
            for i in {1..255}; do echo "192.168.2.$i host-192-168-2-$i-de1"| sudo tee -a /etc/hosts; done

            $SPARK_HOME/sbin/start-slave.sh -m 4g spark://$master_ip:7077
          params:
           $master_ip: {get_attr: [spark-master, first_address]}
    depends_on: 
      - spark-master

###################################################
########## HDFS FILE SERVER #######################
###################################################
hdfs: 
    type: "OS::Nova::Server"
    properties: 
      security_groups: 
        - { get_param: sec_group }
      networks: 
        - network: { get_param: network_id }
      name: G17-hdfs
      flavor: "ssc.medium"
      image: { get_param: image_id }
      availability_zone: nova
      key_name: { get_param: key_name }
      user_data_format: RAW
      user_data: |
        #!/bin/bash
        #Download required packages:
        sudo apt update
        sudo apt install -y openjdk-11-jdk
        cd /home/ubuntu/
        wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz 
        tar -xvzf hadoop-3.3.0.tar.gz 

        #Setup environment variables:
        export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
        export HADOOP_HOME=/home/ubuntu/hadoop-3.3.0
        export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

        #Set up HDFS
        echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh
        mkdir -p /home/ubuntu/hadoopdata/hdfs/namenode 
        mkdir -p /home/ubuntu/hadoopdata/hdfs/datanode

        ##Configure hdfs:
        export INSTANCE_IP=`hostname -I| awk '$1=$1'`
        #core-site.xml
        sed '/^<configuration>$/r'<(
        echo "<property>"
        echo "<name>fs.defaultFS</name>"
        echo "<value>hdfs://$INSTANCE_IP:9000</value>"
        echo "</property>"
        ) -i -- $HADOOP_HOME/etc/hadoop/core-site.xml
        #hdfs-site.xml
        sed '/^<configuration>$/r'<(
        echo "<property>"
        echo "<name>dfs.replication</name>"
        echo "<value>1</value>"
        echo "</property>"
        echo "<property>"
        echo "<name>dfs.name.dir</name>"
        echo "<value>file:////home/ubuntu/hadoopdata/hdfs/namenode</value>"
        echo "</property>"
        echo "<property>"
        echo "<name>dfs.data.dir</name>"
        echo "<value>file:///home/ubuntu/hadoopdata/hdfs/datanode</value>"
        echo "</property>"
        ) -i -- $HADOOP_HOME/etc/hadoop/hdfs-site.xml

        ### WARNING: Following lines does not correctly hence is commentted 
        # #Enable ssh to localhost
        # ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y
        # cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys

        # ## Initiate HDFS
        # #Format namenode
        # hdfs namenode -format <<<y
        # #Start hdfs
        # start-dfs.sh
